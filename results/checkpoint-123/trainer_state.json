{
  "best_global_step": 82,
  "best_metric": 0.5026346445083618,
  "best_model_checkpoint": "results/checkpoint-82",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 123,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.024390243902439025,
      "grad_norm": 10.109831809997559,
      "learning_rate": 0.0002,
      "loss": 0.8106495141983032,
      "step": 1
    },
    {
      "epoch": 0.04878048780487805,
      "grad_norm": 11.067046165466309,
      "learning_rate": 0.0001983739837398374,
      "loss": 0.479077011346817,
      "step": 2
    },
    {
      "epoch": 0.07317073170731707,
      "grad_norm": 4.262631893157959,
      "learning_rate": 0.00019674796747967482,
      "loss": 0.7227815985679626,
      "step": 3
    },
    {
      "epoch": 0.0975609756097561,
      "grad_norm": 3.5753395557403564,
      "learning_rate": 0.0001951219512195122,
      "loss": 0.6665862202644348,
      "step": 4
    },
    {
      "epoch": 0.12195121951219512,
      "grad_norm": 9.50108814239502,
      "learning_rate": 0.00019349593495934962,
      "loss": 0.8383647799491882,
      "step": 5
    },
    {
      "epoch": 0.14634146341463414,
      "grad_norm": 8.757197380065918,
      "learning_rate": 0.000191869918699187,
      "loss": 0.6175611019134521,
      "step": 6
    },
    {
      "epoch": 0.17073170731707318,
      "grad_norm": 14.840785026550293,
      "learning_rate": 0.0001902439024390244,
      "loss": 0.7164916396141052,
      "step": 7
    },
    {
      "epoch": 0.1951219512195122,
      "grad_norm": 3.4776771068573,
      "learning_rate": 0.0001886178861788618,
      "loss": 0.8517243266105652,
      "step": 8
    },
    {
      "epoch": 0.21951219512195122,
      "grad_norm": 2.4558606147766113,
      "learning_rate": 0.00018699186991869918,
      "loss": 0.6721346378326416,
      "step": 9
    },
    {
      "epoch": 0.24390243902439024,
      "grad_norm": 2.4481067657470703,
      "learning_rate": 0.0001853658536585366,
      "loss": 0.6603900194168091,
      "step": 10
    },
    {
      "epoch": 0.2682926829268293,
      "grad_norm": 5.9095234870910645,
      "learning_rate": 0.000183739837398374,
      "loss": 0.6636172533035278,
      "step": 11
    },
    {
      "epoch": 0.2926829268292683,
      "grad_norm": 9.116265296936035,
      "learning_rate": 0.00018211382113821138,
      "loss": 0.7263493537902832,
      "step": 12
    },
    {
      "epoch": 0.3170731707317073,
      "grad_norm": 4.604400634765625,
      "learning_rate": 0.0001804878048780488,
      "loss": 0.7269477248191833,
      "step": 13
    },
    {
      "epoch": 0.34146341463414637,
      "grad_norm": 6.81760835647583,
      "learning_rate": 0.00017886178861788618,
      "loss": 0.6372758746147156,
      "step": 14
    },
    {
      "epoch": 0.36585365853658536,
      "grad_norm": 5.2701592445373535,
      "learning_rate": 0.0001772357723577236,
      "loss": 0.5996804237365723,
      "step": 15
    },
    {
      "epoch": 0.3902439024390244,
      "grad_norm": 3.435281753540039,
      "learning_rate": 0.000175609756097561,
      "loss": 0.5770020484924316,
      "step": 16
    },
    {
      "epoch": 0.4146341463414634,
      "grad_norm": 2.9955577850341797,
      "learning_rate": 0.00017398373983739838,
      "loss": 0.7556462287902832,
      "step": 17
    },
    {
      "epoch": 0.43902439024390244,
      "grad_norm": 2.614231586456299,
      "learning_rate": 0.0001723577235772358,
      "loss": 0.6341516375541687,
      "step": 18
    },
    {
      "epoch": 0.4634146341463415,
      "grad_norm": 4.190999507904053,
      "learning_rate": 0.0001707317073170732,
      "loss": 0.5701944828033447,
      "step": 19
    },
    {
      "epoch": 0.4878048780487805,
      "grad_norm": 9.815690994262695,
      "learning_rate": 0.00016910569105691058,
      "loss": 0.6783714294433594,
      "step": 20
    },
    {
      "epoch": 0.5121951219512195,
      "grad_norm": 5.995128154754639,
      "learning_rate": 0.00016747967479674797,
      "loss": 0.6808998584747314,
      "step": 21
    },
    {
      "epoch": 0.5365853658536586,
      "grad_norm": 2.710401773452759,
      "learning_rate": 0.00016585365853658536,
      "loss": 0.66628098487854,
      "step": 22
    },
    {
      "epoch": 0.5609756097560976,
      "grad_norm": 6.826891899108887,
      "learning_rate": 0.00016422764227642277,
      "loss": 0.7973815202713013,
      "step": 23
    },
    {
      "epoch": 0.5853658536585366,
      "grad_norm": 7.411524295806885,
      "learning_rate": 0.00016260162601626016,
      "loss": 0.7496878504753113,
      "step": 24
    },
    {
      "epoch": 0.6097560975609756,
      "grad_norm": 7.1427693367004395,
      "learning_rate": 0.00016097560975609758,
      "loss": 0.736414909362793,
      "step": 25
    },
    {
      "epoch": 0.6341463414634146,
      "grad_norm": 2.9124326705932617,
      "learning_rate": 0.00015934959349593497,
      "loss": 0.642842173576355,
      "step": 26
    },
    {
      "epoch": 0.6585365853658537,
      "grad_norm": 3.3686037063598633,
      "learning_rate": 0.00015772357723577236,
      "loss": 0.5678421854972839,
      "step": 27
    },
    {
      "epoch": 0.6829268292682927,
      "grad_norm": 13.791831016540527,
      "learning_rate": 0.00015609756097560978,
      "loss": 0.6568776965141296,
      "step": 28
    },
    {
      "epoch": 0.7073170731707317,
      "grad_norm": 3.592717409133911,
      "learning_rate": 0.00015447154471544717,
      "loss": 0.6045543551445007,
      "step": 29
    },
    {
      "epoch": 0.7317073170731707,
      "grad_norm": 6.929385662078857,
      "learning_rate": 0.00015284552845528458,
      "loss": 0.6327217817306519,
      "step": 30
    },
    {
      "epoch": 0.7560975609756098,
      "grad_norm": 5.204023361206055,
      "learning_rate": 0.00015121951219512197,
      "loss": 0.7101250886917114,
      "step": 31
    },
    {
      "epoch": 0.7804878048780488,
      "grad_norm": 2.990781784057617,
      "learning_rate": 0.00014959349593495936,
      "loss": 0.7279523015022278,
      "step": 32
    },
    {
      "epoch": 0.8048780487804879,
      "grad_norm": 3.694549083709717,
      "learning_rate": 0.00014796747967479675,
      "loss": 0.7516610622406006,
      "step": 33
    },
    {
      "epoch": 0.8292682926829268,
      "grad_norm": 3.1634347438812256,
      "learning_rate": 0.00014634146341463414,
      "loss": 0.7732037305831909,
      "step": 34
    },
    {
      "epoch": 0.8536585365853658,
      "grad_norm": 4.521538734436035,
      "learning_rate": 0.00014471544715447156,
      "loss": 0.6389390230178833,
      "step": 35
    },
    {
      "epoch": 0.8780487804878049,
      "grad_norm": 7.572263717651367,
      "learning_rate": 0.00014308943089430895,
      "loss": 0.7142578959465027,
      "step": 36
    },
    {
      "epoch": 0.9024390243902439,
      "grad_norm": 13.04961109161377,
      "learning_rate": 0.00014146341463414634,
      "loss": 0.6616743803024292,
      "step": 37
    },
    {
      "epoch": 0.926829268292683,
      "grad_norm": 7.293030261993408,
      "learning_rate": 0.00013983739837398375,
      "loss": 0.5197018384933472,
      "step": 38
    },
    {
      "epoch": 0.9512195121951219,
      "grad_norm": 6.761922836303711,
      "learning_rate": 0.00013821138211382114,
      "loss": 0.5981365442276001,
      "step": 39
    },
    {
      "epoch": 0.975609756097561,
      "grad_norm": 6.090510845184326,
      "learning_rate": 0.00013658536585365856,
      "loss": 0.7853691577911377,
      "step": 40
    },
    {
      "epoch": 1.0,
      "grad_norm": 18.39741325378418,
      "learning_rate": 0.00013495934959349595,
      "loss": 0.9012986421585083,
      "step": 41
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.6097560975609756,
      "eval_loss": 0.6383993625640869,
      "eval_runtime": 3.1506,
      "eval_samples_per_second": 13.013,
      "eval_steps_per_second": 3.491,
      "step": 41
    },
    {
      "epoch": 1.024390243902439,
      "grad_norm": 2.6196885108947754,
      "learning_rate": 0.00013333333333333334,
      "loss": 0.6159932613372803,
      "step": 42
    },
    {
      "epoch": 1.048780487804878,
      "grad_norm": 3.865347146987915,
      "learning_rate": 0.00013170731707317076,
      "loss": 0.5965090394020081,
      "step": 43
    },
    {
      "epoch": 1.0731707317073171,
      "grad_norm": 6.6009392738342285,
      "learning_rate": 0.00013008130081300815,
      "loss": 0.720150887966156,
      "step": 44
    },
    {
      "epoch": 1.0975609756097562,
      "grad_norm": 4.831294536590576,
      "learning_rate": 0.00012845528455284554,
      "loss": 0.42271506786346436,
      "step": 45
    },
    {
      "epoch": 1.1219512195121952,
      "grad_norm": 7.745965957641602,
      "learning_rate": 0.00012682926829268293,
      "loss": 0.5283469557762146,
      "step": 46
    },
    {
      "epoch": 1.146341463414634,
      "grad_norm": 7.911777496337891,
      "learning_rate": 0.00012520325203252032,
      "loss": 0.6770191192626953,
      "step": 47
    },
    {
      "epoch": 1.170731707317073,
      "grad_norm": 5.1865715980529785,
      "learning_rate": 0.00012357723577235773,
      "loss": 0.5657308101654053,
      "step": 48
    },
    {
      "epoch": 1.1951219512195121,
      "grad_norm": 3.4220130443573,
      "learning_rate": 0.00012195121951219512,
      "loss": 0.5063868761062622,
      "step": 49
    },
    {
      "epoch": 1.2195121951219512,
      "grad_norm": 15.335349082946777,
      "learning_rate": 0.00012032520325203254,
      "loss": 0.8691784739494324,
      "step": 50
    },
    {
      "epoch": 1.2439024390243902,
      "grad_norm": 8.517654418945312,
      "learning_rate": 0.00011869918699186993,
      "loss": 0.5969027280807495,
      "step": 51
    },
    {
      "epoch": 1.2682926829268293,
      "grad_norm": 8.967811584472656,
      "learning_rate": 0.00011707317073170732,
      "loss": 0.882971465587616,
      "step": 52
    },
    {
      "epoch": 1.2926829268292683,
      "grad_norm": 3.9210076332092285,
      "learning_rate": 0.00011544715447154472,
      "loss": 0.5526942610740662,
      "step": 53
    },
    {
      "epoch": 1.3170731707317074,
      "grad_norm": 3.9385507106781006,
      "learning_rate": 0.00011382113821138211,
      "loss": 0.6784436702728271,
      "step": 54
    },
    {
      "epoch": 1.3414634146341464,
      "grad_norm": 2.6303727626800537,
      "learning_rate": 0.00011219512195121953,
      "loss": 0.5234106779098511,
      "step": 55
    },
    {
      "epoch": 1.3658536585365852,
      "grad_norm": 3.1057074069976807,
      "learning_rate": 0.00011056910569105692,
      "loss": 0.5631933808326721,
      "step": 56
    },
    {
      "epoch": 1.3902439024390243,
      "grad_norm": 4.450168609619141,
      "learning_rate": 0.00010894308943089431,
      "loss": 0.4234226644039154,
      "step": 57
    },
    {
      "epoch": 1.4146341463414633,
      "grad_norm": 3.2539162635803223,
      "learning_rate": 0.00010731707317073172,
      "loss": 0.4985968768596649,
      "step": 58
    },
    {
      "epoch": 1.4390243902439024,
      "grad_norm": 9.75235366821289,
      "learning_rate": 0.00010569105691056911,
      "loss": 0.4421592950820923,
      "step": 59
    },
    {
      "epoch": 1.4634146341463414,
      "grad_norm": 4.100281238555908,
      "learning_rate": 0.00010406504065040652,
      "loss": 0.3806692361831665,
      "step": 60
    },
    {
      "epoch": 1.4878048780487805,
      "grad_norm": 10.534725189208984,
      "learning_rate": 0.0001024390243902439,
      "loss": 1.0763766765594482,
      "step": 61
    },
    {
      "epoch": 1.5121951219512195,
      "grad_norm": 9.521418571472168,
      "learning_rate": 0.0001008130081300813,
      "loss": 0.7926416397094727,
      "step": 62
    },
    {
      "epoch": 1.5365853658536586,
      "grad_norm": 5.75792121887207,
      "learning_rate": 9.91869918699187e-05,
      "loss": 0.5426416397094727,
      "step": 63
    },
    {
      "epoch": 1.5609756097560976,
      "grad_norm": 4.555993556976318,
      "learning_rate": 9.75609756097561e-05,
      "loss": 0.40343159437179565,
      "step": 64
    },
    {
      "epoch": 1.5853658536585367,
      "grad_norm": 3.4619181156158447,
      "learning_rate": 9.59349593495935e-05,
      "loss": 0.3415723443031311,
      "step": 65
    },
    {
      "epoch": 1.6097560975609757,
      "grad_norm": 4.57151460647583,
      "learning_rate": 9.43089430894309e-05,
      "loss": 0.32148951292037964,
      "step": 66
    },
    {
      "epoch": 1.6341463414634148,
      "grad_norm": 11.82491397857666,
      "learning_rate": 9.26829268292683e-05,
      "loss": 0.6480188369750977,
      "step": 67
    },
    {
      "epoch": 1.6585365853658538,
      "grad_norm": 3.949143409729004,
      "learning_rate": 9.105691056910569e-05,
      "loss": 0.3360965847969055,
      "step": 68
    },
    {
      "epoch": 1.6829268292682928,
      "grad_norm": 2.818089246749878,
      "learning_rate": 8.943089430894309e-05,
      "loss": 0.530596911907196,
      "step": 69
    },
    {
      "epoch": 1.7073170731707317,
      "grad_norm": 10.532952308654785,
      "learning_rate": 8.78048780487805e-05,
      "loss": 0.7829866409301758,
      "step": 70
    },
    {
      "epoch": 1.7317073170731707,
      "grad_norm": 8.45717716217041,
      "learning_rate": 8.61788617886179e-05,
      "loss": 0.3759872019290924,
      "step": 71
    },
    {
      "epoch": 1.7560975609756098,
      "grad_norm": 10.994580268859863,
      "learning_rate": 8.455284552845529e-05,
      "loss": 0.6184077262878418,
      "step": 72
    },
    {
      "epoch": 1.7804878048780488,
      "grad_norm": 6.25429105758667,
      "learning_rate": 8.292682926829268e-05,
      "loss": 0.5713375806808472,
      "step": 73
    },
    {
      "epoch": 1.8048780487804879,
      "grad_norm": 4.47775411605835,
      "learning_rate": 8.130081300813008e-05,
      "loss": 0.3289045989513397,
      "step": 74
    },
    {
      "epoch": 1.8292682926829267,
      "grad_norm": 5.817636966705322,
      "learning_rate": 7.967479674796748e-05,
      "loss": 0.3641018569469452,
      "step": 75
    },
    {
      "epoch": 1.8536585365853657,
      "grad_norm": 5.956912040710449,
      "learning_rate": 7.804878048780489e-05,
      "loss": 0.4121873676776886,
      "step": 76
    },
    {
      "epoch": 1.8780487804878048,
      "grad_norm": 8.863551139831543,
      "learning_rate": 7.642276422764229e-05,
      "loss": 0.9137557148933411,
      "step": 77
    },
    {
      "epoch": 1.9024390243902438,
      "grad_norm": 6.613606929779053,
      "learning_rate": 7.479674796747968e-05,
      "loss": 0.3441274166107178,
      "step": 78
    },
    {
      "epoch": 1.9268292682926829,
      "grad_norm": 3.515960216522217,
      "learning_rate": 7.317073170731707e-05,
      "loss": 0.24798345565795898,
      "step": 79
    },
    {
      "epoch": 1.951219512195122,
      "grad_norm": 3.5714855194091797,
      "learning_rate": 7.154471544715447e-05,
      "loss": 0.3837624788284302,
      "step": 80
    },
    {
      "epoch": 1.975609756097561,
      "grad_norm": 3.5895678997039795,
      "learning_rate": 6.991869918699188e-05,
      "loss": 0.37475234270095825,
      "step": 81
    },
    {
      "epoch": 2.0,
      "grad_norm": 5.544576644897461,
      "learning_rate": 6.829268292682928e-05,
      "loss": 0.21068209409713745,
      "step": 82
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.7804878048780488,
      "eval_loss": 0.5026346445083618,
      "eval_runtime": 3.5395,
      "eval_samples_per_second": 11.584,
      "eval_steps_per_second": 3.108,
      "step": 82
    },
    {
      "epoch": 2.024390243902439,
      "grad_norm": 6.546365261077881,
      "learning_rate": 6.666666666666667e-05,
      "loss": 0.3142501711845398,
      "step": 83
    },
    {
      "epoch": 2.048780487804878,
      "grad_norm": 3.5644547939300537,
      "learning_rate": 6.504065040650407e-05,
      "loss": 0.23297828435897827,
      "step": 84
    },
    {
      "epoch": 2.073170731707317,
      "grad_norm": 6.627591609954834,
      "learning_rate": 6.341463414634146e-05,
      "loss": 0.4798732399940491,
      "step": 85
    },
    {
      "epoch": 2.097560975609756,
      "grad_norm": 4.907685279846191,
      "learning_rate": 6.178861788617887e-05,
      "loss": 0.36827054619789124,
      "step": 86
    },
    {
      "epoch": 2.1219512195121952,
      "grad_norm": 17.54926872253418,
      "learning_rate": 6.016260162601627e-05,
      "loss": 1.1110684871673584,
      "step": 87
    },
    {
      "epoch": 2.1463414634146343,
      "grad_norm": 3.6839990615844727,
      "learning_rate": 5.853658536585366e-05,
      "loss": 0.2364988923072815,
      "step": 88
    },
    {
      "epoch": 2.1707317073170733,
      "grad_norm": 4.493812561035156,
      "learning_rate": 5.6910569105691056e-05,
      "loss": 0.2035362720489502,
      "step": 89
    },
    {
      "epoch": 2.1951219512195124,
      "grad_norm": 3.461125373840332,
      "learning_rate": 5.528455284552846e-05,
      "loss": 0.23448222875595093,
      "step": 90
    },
    {
      "epoch": 2.2195121951219514,
      "grad_norm": 8.639632225036621,
      "learning_rate": 5.365853658536586e-05,
      "loss": 0.3195556700229645,
      "step": 91
    },
    {
      "epoch": 2.2439024390243905,
      "grad_norm": 7.352189064025879,
      "learning_rate": 5.203252032520326e-05,
      "loss": 0.6254689693450928,
      "step": 92
    },
    {
      "epoch": 2.2682926829268295,
      "grad_norm": 4.687083721160889,
      "learning_rate": 5.040650406504065e-05,
      "loss": 0.25024479627609253,
      "step": 93
    },
    {
      "epoch": 2.292682926829268,
      "grad_norm": 6.150921821594238,
      "learning_rate": 4.878048780487805e-05,
      "loss": 0.43805480003356934,
      "step": 94
    },
    {
      "epoch": 2.317073170731707,
      "grad_norm": 5.098286151885986,
      "learning_rate": 4.715447154471545e-05,
      "loss": 0.5849214196205139,
      "step": 95
    },
    {
      "epoch": 2.341463414634146,
      "grad_norm": 10.027630805969238,
      "learning_rate": 4.5528455284552844e-05,
      "loss": 1.2403868436813354,
      "step": 96
    },
    {
      "epoch": 2.3658536585365852,
      "grad_norm": 7.4930267333984375,
      "learning_rate": 4.390243902439025e-05,
      "loss": 0.34362784028053284,
      "step": 97
    },
    {
      "epoch": 2.3902439024390243,
      "grad_norm": 6.17583703994751,
      "learning_rate": 4.2276422764227644e-05,
      "loss": 0.2415829598903656,
      "step": 98
    },
    {
      "epoch": 2.4146341463414633,
      "grad_norm": 4.244898796081543,
      "learning_rate": 4.065040650406504e-05,
      "loss": 0.3598310947418213,
      "step": 99
    },
    {
      "epoch": 2.4390243902439024,
      "grad_norm": 3.436119556427002,
      "learning_rate": 3.9024390243902444e-05,
      "loss": 0.21036450564861298,
      "step": 100
    },
    {
      "epoch": 2.4634146341463414,
      "grad_norm": 3.999830722808838,
      "learning_rate": 3.739837398373984e-05,
      "loss": 0.2822798788547516,
      "step": 101
    },
    {
      "epoch": 2.4878048780487805,
      "grad_norm": 2.6753733158111572,
      "learning_rate": 3.577235772357724e-05,
      "loss": 0.13418357074260712,
      "step": 102
    },
    {
      "epoch": 2.5121951219512195,
      "grad_norm": 11.614812850952148,
      "learning_rate": 3.414634146341464e-05,
      "loss": 0.9552189111709595,
      "step": 103
    },
    {
      "epoch": 2.5365853658536586,
      "grad_norm": 2.081341505050659,
      "learning_rate": 3.2520325203252037e-05,
      "loss": 0.13562655448913574,
      "step": 104
    },
    {
      "epoch": 2.5609756097560976,
      "grad_norm": 10.176401138305664,
      "learning_rate": 3.089430894308943e-05,
      "loss": 0.446052223443985,
      "step": 105
    },
    {
      "epoch": 2.5853658536585367,
      "grad_norm": 4.5876617431640625,
      "learning_rate": 2.926829268292683e-05,
      "loss": 0.4644136130809784,
      "step": 106
    },
    {
      "epoch": 2.6097560975609757,
      "grad_norm": 4.206441402435303,
      "learning_rate": 2.764227642276423e-05,
      "loss": 0.5646387934684753,
      "step": 107
    },
    {
      "epoch": 2.6341463414634148,
      "grad_norm": 10.60500431060791,
      "learning_rate": 2.601626016260163e-05,
      "loss": 0.6794250011444092,
      "step": 108
    },
    {
      "epoch": 2.658536585365854,
      "grad_norm": 2.861471176147461,
      "learning_rate": 2.4390243902439026e-05,
      "loss": 0.20125070214271545,
      "step": 109
    },
    {
      "epoch": 2.682926829268293,
      "grad_norm": 6.068546772003174,
      "learning_rate": 2.2764227642276422e-05,
      "loss": 0.6160800457000732,
      "step": 110
    },
    {
      "epoch": 2.7073170731707314,
      "grad_norm": 6.153824329376221,
      "learning_rate": 2.1138211382113822e-05,
      "loss": 0.41659244894981384,
      "step": 111
    },
    {
      "epoch": 2.7317073170731705,
      "grad_norm": 2.5972676277160645,
      "learning_rate": 1.9512195121951222e-05,
      "loss": 0.14592109620571136,
      "step": 112
    },
    {
      "epoch": 2.7560975609756095,
      "grad_norm": 10.634321212768555,
      "learning_rate": 1.788617886178862e-05,
      "loss": 0.9366727471351624,
      "step": 113
    },
    {
      "epoch": 2.7804878048780486,
      "grad_norm": 12.352066993713379,
      "learning_rate": 1.6260162601626018e-05,
      "loss": 1.524552345275879,
      "step": 114
    },
    {
      "epoch": 2.8048780487804876,
      "grad_norm": 3.6015305519104004,
      "learning_rate": 1.4634146341463415e-05,
      "loss": 0.18040040135383606,
      "step": 115
    },
    {
      "epoch": 2.8292682926829267,
      "grad_norm": 12.25301742553711,
      "learning_rate": 1.3008130081300815e-05,
      "loss": 1.0367387533187866,
      "step": 116
    },
    {
      "epoch": 2.8536585365853657,
      "grad_norm": 5.586702823638916,
      "learning_rate": 1.1382113821138211e-05,
      "loss": 0.2174505889415741,
      "step": 117
    },
    {
      "epoch": 2.8780487804878048,
      "grad_norm": 3.1526200771331787,
      "learning_rate": 9.756097560975611e-06,
      "loss": 0.15743184089660645,
      "step": 118
    },
    {
      "epoch": 2.902439024390244,
      "grad_norm": 12.0233793258667,
      "learning_rate": 8.130081300813009e-06,
      "loss": 1.6206073760986328,
      "step": 119
    },
    {
      "epoch": 2.926829268292683,
      "grad_norm": 2.8699731826782227,
      "learning_rate": 6.504065040650407e-06,
      "loss": 0.1632559895515442,
      "step": 120
    },
    {
      "epoch": 2.951219512195122,
      "grad_norm": 11.348984718322754,
      "learning_rate": 4.8780487804878055e-06,
      "loss": 0.8860393762588501,
      "step": 121
    },
    {
      "epoch": 2.975609756097561,
      "grad_norm": 9.243736267089844,
      "learning_rate": 3.2520325203252037e-06,
      "loss": 0.5772106647491455,
      "step": 122
    },
    {
      "epoch": 3.0,
      "grad_norm": 4.870321273803711,
      "learning_rate": 1.6260162601626018e-06,
      "loss": 0.18156273663043976,
      "step": 123
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.6585365853658537,
      "eval_loss": 0.650522768497467,
      "eval_runtime": 3.3109,
      "eval_samples_per_second": 12.383,
      "eval_steps_per_second": 3.322,
      "step": 123
    }
  ],
  "logging_steps": 1,
  "max_steps": 123,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 63980044240896.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
